{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making necessary imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from datetime import datetime\n",
    "import random\n",
    "import time\n",
    "from hmmlearn import hmm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the dataset\n",
    "djia_dataset = pd.read_csv(\"DJA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "djia_dataset = djia_dataset.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "djia_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the dataset\n",
    "snp = pd.read_csv(\"SNP.csv\")\n",
    "snp = snp[1075:1829]\n",
    "nasdaq = pd.read_csv(\"NASDAQCOM.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual trend of data\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.plot(djia_dataset[\"DJIA\"].tolist(), label='DJIA')\n",
    "plt.plot(nasdaq[\"Close\"].tolist(), label='NASDAQ')\n",
    "plt.plot(snp[\"Close/Last\"].tolist(), label='SNP500')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Stock Close Price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the variation\n",
    "def find_variation(close_prices):\n",
    "    variation = [0]\n",
    "    for i in range(1, len(close_prices)):\n",
    "        variation.append(((close_prices[i] - close_prices[i-1])/close_prices[i])*100)\n",
    "    return variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating and displaying variation\n",
    "djia_variation = find_variation(djia_dataset[\"DJIA\"].tolist())\n",
    "snp_variation = find_variation(snp[\"Close/Last\"].tolist())\n",
    "nasdaq_variation = find_variation(nasdaq[\"Close\"].tolist())\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "plt.plot(djia_variation, label = 'DJIA Close Price Variation')\n",
    "plt.plot(nasdaq_variation, label = 'NASDAQ Close Price Variation')\n",
    "plt.plot(snp_variation, label='SNP Close Price Variation')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Time Period/Date')\n",
    "plt.ylabel('Variation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate random transition matrix and starting probability\n",
    "\n",
    "def generate_random_transition_matrix():\n",
    "    k = 8\n",
    "    result = [[random.uniform(0, 0.1 / k) for i in range(k)] for j in range(k)]\n",
    "    for j, r in enumerate(result):\n",
    "        r[j] += 1 - sum(r)\n",
    "    return np.asarray(result)\n",
    "def generate_random_start_prob():\n",
    "    k = 8\n",
    "    start_prob = [np.random.uniform(0,1) for _ in range(k)]\n",
    "    start_prob = np.asarray(start_prob)\n",
    "    start_prob = start_prob/np.sum(start_prob)\n",
    "    return np.asarray(start_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSP Solver\n",
    "from queue import PriorityQueue\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class Constraint(ABC):\n",
    "    def __init__(self, variables):\n",
    "        self.variables = variables\n",
    "\n",
    "    @abstractmethod\n",
    "    def satisfied(self, assignment):\n",
    "        pass\n",
    "\n",
    "\n",
    "class CSP():\n",
    "    def __init__(self, variables, domains):\n",
    "        self.variables = variables\n",
    "        self.domains = domains\n",
    "        self.constraints = {}\n",
    "        for variable in self.variables:\n",
    "            self.constraints[variable] = []\n",
    "            if variable not in self.domains:\n",
    "                raise LookupError(\n",
    "                    'Every variable should have a domain assigned to it.')\n",
    "\n",
    "    def add_constraint(self, constraint):\n",
    "        for variable in constraint.variables:\n",
    "            if variable not in self.variables:\n",
    "                raise LookupError(\"Variable in constraint not in CSP\")\n",
    "            else:\n",
    "                self.constraints[variable].append(constraint)\n",
    "\n",
    "    def consistent(self, variable, assignment):\n",
    "        for constraint in self.constraints[variable]:\n",
    "            if not constraint.satisfied(assignment):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def backtracking_search(self, assignment={}):\n",
    "        # assignment is complete if every variable is assigned (our base case)\n",
    "        if len(assignment) == len(self.variables):\n",
    "            return assignment\n",
    "        # get all variables in the CSP but not in the assignment\n",
    "        unassigned = [v for v in self.variables if v not in assignment]\n",
    "        first = unassigned[0]\n",
    "        for value in self.domains[first]:\n",
    "            local_assignment = assignment.copy()\n",
    "            local_assignment[first] = value\n",
    "            # if we're still consistent, we recurse (continue)\n",
    "            if self.consistent(first, local_assignment):\n",
    "                result = self.backtracking_search(local_assignment)\n",
    "                if result is not None:\n",
    "                    return result\n",
    "        return None\n",
    "\n",
    "\n",
    "class Constraint(Constraint):\n",
    "    def __init__(self, place1, place2):\n",
    "        super().__init__([place1, place2])\n",
    "        self.place1 = place1\n",
    "        self.place2 = place2\n",
    "\n",
    "    def satisfied(self, assignment):\n",
    "        if self.place1 not in assignment or self.place2 not in assignment:\n",
    "            return True\n",
    "        return assignment[self.place1] != assignment[self.place2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trump Tweet Sentiment Analysis\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "words = set(nltk.corpus.words.words())\n",
    "real_donald_trump = pd.read_csv(\"realdonaldtrump.csv\")\n",
    "trump_tweets = pd.read_csv(\"trumptweets.csv\")\n",
    "cond1 = real_donald_trump['date'] >= \"2017-01-20\"\n",
    "cond2 = real_donald_trump['date'] <= \"2020-01-20\"\n",
    "real_donald_trump = real_donald_trump.where((cond1 & cond2)).dropna()\n",
    "def cleaner(tweet):\n",
    "    tweet = re.sub(\"@[A-Za-z0-9]+\",\"\",tweet) #Remove @ sign\n",
    "    tweet = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", tweet) #Remove http links\n",
    "    tweet = \" \".join(tweet.split())\n",
    "    tweet = tweet.replace(\"#\", \"\").replace(\"_\", \" \") #Remove hashtag sign but keep the text\n",
    "    tweet = \" \".join(w for w in nltk.wordpunct_tokenize(tweet)\n",
    "         if w.lower() in words or not w.isalpha())\n",
    "    return tweet\n",
    "    \n",
    "\n",
    "real_donald_trump['content_clean'] = real_donald_trump['content'].apply(cleaner)\n",
    "word_dict = {'manipulate':-1,'manipulative':-1,'jamescharlesiscancelled':-1,'jamescharlesisoverparty':-1,\n",
    "            'pedophile':-1,'pedo':-1,'cancel':-1,'cancelled':-1,'cancel culture':0.4,'teamtati':-1,'teamjames':1,\n",
    "            'teamjamescharles':1,'liar':-1,'MAGA':-1}\n",
    "\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "sid.lexicon.update(word_dict)\n",
    "\n",
    "list1 = []\n",
    "for i in real_donald_trump['content_clean']:\n",
    "    list1.append((sid.polarity_scores(str(i)))['compound'])\n",
    "real_donald_trump['sentiment'] = list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a dictionary of states\n",
    "states_dict = {\n",
    "    0: 'very-small-rise',\n",
    "    1: 'small-rise',\n",
    "    2: 'large-rise',\n",
    "    3: 'very-large-rise',\n",
    "    4: 'very-small-drop',\n",
    "    5: 'small-drop',\n",
    "    6: 'large-drop',\n",
    "    7: 'very-large-drop',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train HMM without CSP\n",
    "def training_hmm_without_csp(training_data):\n",
    "    start_prob = generate_random_start_prob()\n",
    "    transition_matrix = generate_random_transition_matrix()\n",
    "    model = hmm.GMMHMM(n_components=8, n_mix = 7, covariance_type=\"diag\", n_iter=10)\n",
    "    model.fit(training_data)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train HMM with CSP\n",
    "def training_hmm_with_csp(training_data, tweet_dataset):\n",
    "    start_prob = generate_random_start_prob()\n",
    "    transition_matrix = generate_random_transition_matrix()\n",
    "\n",
    "    variables = [\n",
    "    \"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"\n",
    "    ]\n",
    "    domains = {\n",
    "        \"0\":[\"0\",\"1\"],\n",
    "        \"1\":[\"0\",\"1\"],\n",
    "        \"2\":[\"0\",\"1\"],\n",
    "        \"3\":[\"0\",\"1\"],\n",
    "        \"4\":[\"0\",\"1\"],\n",
    "        \"5\":[\"0\",\"1\"],\n",
    "        \"6\":[\"0\",\"1\"],\n",
    "        \"7\":[\"0\",\"1\"],\n",
    "        \"8\":[\"0\",\"1\"],\n",
    "    }\n",
    "    csp = CSP(variables, domains)\n",
    "\n",
    "    tweet_sentiment = tweet_dataset[\"sentiment\"].tolist()\n",
    "    # Adding constraints to our model\n",
    "    for i in range(len(tweet_sentiment)):\n",
    "        if(tweet_sentiment[i] >= 0.9): csp.add_constraint(Constraint(\"4\",\"3\"))\n",
    "        if(tweet_sentiment[i] >= 0.6 and tweet_sentiment[i] < 0.9): csp.add_constraint(Constraint(\"1\",\"2\"))\n",
    "        if(tweet_sentiment[i] >= 0.3 and tweet_sentiment[i] < 0.1): csp.add_constraint(Constraint(\"4\",\"5\"))\n",
    "        else: csp.add_constraint(Constraint(\"6\",\"7\"))\n",
    "\n",
    "    solution = csp.backtracking_search()\n",
    "    transition_matrix = np.identity(8)\n",
    "    model = hmm.GMMHMM(n_components=8, n_mix = 7, covariance_type=\"diag\", n_iter=10)\n",
    "    model.transmat_ = transition_matrix\n",
    "    model.fit(training_data)\n",
    "\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the index of states (hidden states)\n",
    "def calculate_index_states(model, X_test):\n",
    "    X_test = np.asarray(X_test).reshape(-1,1)\n",
    "    index_states = model.predict(X_test)\n",
    "    return index_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify the actual variation of each state (Index)\n",
    "def classify_state_variation(variations):\n",
    "    states = []\n",
    "    for variation in variations:\n",
    "        if(0 <= variation and variation < 0.1):\n",
    "            states.append(0)\n",
    "        elif(0.1 <= variation and variation < 1):\n",
    "            states.append(1)\n",
    "        elif(1 <= variation and variation < 2):\n",
    "            states.append(2)\n",
    "        elif(variation >= 2):\n",
    "            states.append(3)\n",
    "        elif(-0.1 < variation and variation <= 0):\n",
    "            states.append(4)\n",
    "        elif(-1 < variation and variation <= -0.1):\n",
    "            states.append(5)\n",
    "        elif(-2 < variation and variation <= -1):\n",
    "            states.append(6)\n",
    "        elif(variation <= -2):\n",
    "            states.append(7)\n",
    "    return states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Classify the actual variation of each state (Index)\n",
    "\n",
    "djia_states = classify_state_variation(djia_variation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing best out of 100 models with out CSP\n",
    "def choose_best_model_without_csp(data):\n",
    "    X = data\n",
    "    X_train = X[:int(0.8*len(X))]\n",
    "    X_test = X[int(0.8*len(X)):]\n",
    "    X_train = np.asarray(X_train).reshape(-1, 1)\n",
    "    X_test = np.asarray(X_test).reshape(-1, 1)\n",
    "    model = training_hmm_without_csp(X_train)\n",
    "    best_model = model.fit(X_train)\n",
    "    best_model_score = best_model.score(X_test)\n",
    "    print(\"Model 1: Score = \" + str(best_model_score))\n",
    "    for i in range(2,101):\n",
    "        model = training_hmm_without_csp(X_train)\n",
    "        print(\"Model \" + str(i) + \": Score = \" + str(model.score(X_test)))\n",
    "        if(model.score(X_test) >= best_model_score):\n",
    "            best_model_score = model.score(X_test)\n",
    "            best_model = model\n",
    "\n",
    "    print(\"Best Model Found - Score = \" + str(best_model_score))\n",
    "    return best_model, best_model_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing best out of 100 models with CSP\n",
    "def choose_best_model_with_csp(data, tweet_dataset):\n",
    "    X = data\n",
    "    X_train = X[:int(0.8*len(X))]\n",
    "    X_test = X[int(0.8*len(X)):]\n",
    "    X_train = np.asarray(X_train).reshape(-1, 1)\n",
    "    X_test = np.asarray(X_test).reshape(-1, 1)\n",
    "    model = training_hmm_without_csp(X_train)\n",
    "    best_model = model.fit(X_train)\n",
    "    best_model_score = best_model.score(X_test)\n",
    "    print(\"Model 1: Score = \" + str(best_model_score))\n",
    "    for i in range(2,101):\n",
    "        model = training_hmm_with_csp(X_train, tweet_dataset)\n",
    "        print(\"Model \" + str(i) + \": Score = \" + str(model.score(X_test)))\n",
    "        if(model.score(X_test) >= best_model_score):\n",
    "            best_model_score = model.score(X_test)\n",
    "            best_model = model\n",
    "\n",
    "    print(\"Best Model Found - Score = \" + str(best_model_score))\n",
    "    return best_model, best_model_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate best model params and score\n",
    "djia_dataset_normalized = np.asarray(djia_dataset[\"DJIA\"].tolist())\n",
    "djia_dataset_normalized = djia_dataset_normalized/sum(djia_dataset_normalized)\n",
    "djia_dataset_normalized = djia_dataset_normalized.tolist()\n",
    "\n",
    "best_model_without_csp, best_model_without_csp_score = choose_best_model_without_csp(djia_dataset_normalized)\n",
    "best_model_with_csp, best_model_with_csp_score = choose_best_model_with_csp(djia_dataset_normalized, real_donald_trump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate index of states\n",
    "index_states_without_csp = calculate_index_states(best_model_without_csp, djia_dataset_normalized[int(0.8*len(djia_dataset_normalized)):])\n",
    "index_states_with_csp = calculate_index_states(best_model_with_csp, djia_dataset_normalized[int(0.8*len(djia_dataset_normalized)):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "djia_states_testing = djia_states[int(0.8*len(djia_dataset_normalized)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count correct predictions\n",
    "def correct_predictions(actual_states, calculated_states):\n",
    "    count = 0\n",
    "    for i in range(len(actual_states)):\n",
    "        if(abs(actual_states[i] - calculated_states[i]) <= 3):\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Accuracy\n",
    "accuracy_without_csp = correct_predictions(djia_states_testing, index_states_without_csp)/len(djia_states_testing)\n",
    "accuracy_with_csp = correct_predictions(djia_states_testing, index_states_with_csp)/len(djia_states_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_with_csp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MAPE\n",
    "def calculate_mape(actual_states, calculated_states):\n",
    "    count = 0\n",
    "    for i in range(len(actual_states)):\n",
    "        count = count + abs((actual_states[i] - calculated_states[i])/(actual_states[i]+100))\n",
    "    return (count/len(calculated_states))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_mape(djia_states_testing, index_states_with_csp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Prices\n",
    "def predict_prices(data, index_states):\n",
    "    X = data\n",
    "    price_variation = [-3,-2,-1,-0.1,0.1,1,2,3]\n",
    "    predicted_prices = [X[int(0.8*len(X))-1]]\n",
    "    for i in range(1, int(0.2*len(X))):\n",
    "        new_price = predicted_prices[i-1] + predicted_prices[i-1]*price_variation[index_states[i]]/100\n",
    "        predicted_prices.append(new_price)\n",
    "    return predicted_prices    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_prices_without_csp = predict_prices(djia_dataset_normalized, index_states_without_csp)\n",
    "predict_prices_with_csp = predict_prices(djia_dataset_normalized, index_states_with_csp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State Transition\n",
    "#fig = plt.figure(figsize=(10,5))\n",
    "fig, ax = plt.subplots(2)\n",
    "ax[0].plot(index_states_without_csp, label = 'State transition without CSP')\n",
    "ax[1].plot(index_states_with_csp,label = 'State transition with CSP', color='green')\n",
    "ax[0].set_xlabel(\"Time\")\n",
    "ax[0].set_ylabel(\"State\")\n",
    "ax[1].set_xlabel(\"Time\")\n",
    "ax[1].set_ylabel(\"State\")\n",
    "ax[0].set_title('State transition without and with CSP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predicted trend vs actual trend\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.plot(predict_prices_without_csp, label = 'Predicted Values - Without CSP')\n",
    "plt.plot(predict_prices_with_csp, label = 'Predicted Values - With CSP')\n",
    "plt.plot(djia_dataset_normalized[int(0.8*len(djia_dataset_normalized)):], label = 'Actual Trend')\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"DJIA Index Value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#with open(\"bestfitdjiamodeln.pkl\", \"wb\") as f: \n",
    "#    pickle.dump(best_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = djia_dataset[\"DJIA\"].tolist()\n",
    "X_train = X[:int(0.8*len(X))]\n",
    "X_test = X[int(0.8*len(X)):]\n",
    "X_train = np.asarray(X_train).reshape(-1, 1)\n",
    "X_test = np.asarray(X_test).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find time complexity\n",
    "def finding_model_time(X_train, tweet_dataset):\n",
    "    start_time = time.time()\n",
    "    model = training_hmm_without_csp(X_train)\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    model = training_hmm_with_csp(X_train, tweet_dataset)\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finding_model_time(X_train, real_donald_trump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "74896034db1845be5000ed9314487ee297f444d1b9bf6f172497bb9704b8efa0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
